{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisits\n",
    "\n",
    "* make sure to download and unzip the data files (train,test,test_labels) from [this link]()\n",
    "* it's recommended that you creata a new `conda environment` by running `conda env --name ENV_NAME` replacing the `ENV_NAME` with a name of your choice, and install the `requirments.txt` by executing `pip install -r requirments.txt`\n",
    "* ℹ️ if you don't want to train the model from scratch, download the `pretrained_bilstm_attn` model from [here](https://drive.google.com/file/d/1NsO26I_VTvnqiJFAVA8J7zECZCB8ONI7/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explanatory Data Analysis\n",
    "\n",
    "visualize the data and understand the distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### importing the data from the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "y_true = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### have an idea about the length of the comments strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['comment_text'].apply(lambda x:len(x.split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the column names for the target variables \n",
    "these will be used later to create the train,dev and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_target = train_df.columns[2:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data shape and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40740</th>\n",
       "      <td>6cbb5277a91ed077</td>\n",
       "      <td>Mr. Viligante! \\n\\nDear Mr. Ioeth whateverthef...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103830</th>\n",
       "      <td>2b813ab782d34984</td>\n",
       "      <td>Important note \\n\\nDIE.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139458</th>\n",
       "      <td>ea714f84e062c442</td>\n",
       "      <td>The article did include his real name and it i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69633</th>\n",
       "      <td>ba46606ee091e83b</td>\n",
       "      <td>\"\\nThx. Someone put it on Family of Barack Oba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128559</th>\n",
       "      <td>af92f1e2f4c42372</td>\n",
       "      <td>\"\\nI apoligize for not being able to be \"\"full...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107815</th>\n",
       "      <td>4043aec662bda17c</td>\n",
       "      <td>listen peeps................this guy is one of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71291</th>\n",
       "      <td>bed529ce6cb780a8</td>\n",
       "      <td>This addition is just as misleading as the sum...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156348</th>\n",
       "      <td>cccd45d35c7cd352</td>\n",
       "      <td>your tendancies to be editor of financial topi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153271</th>\n",
       "      <td>9a204083856a48c1</td>\n",
       "      <td>fair enough. i'll go hide in a hole. good luck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91919</th>\n",
       "      <td>f5bc78ba654ab185</td>\n",
       "      <td>Nagorno Karabakh \\n\\nThe following resource de...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "40740   6cbb5277a91ed077  Mr. Viligante! \\n\\nDear Mr. Ioeth whateverthef...   \n",
       "103830  2b813ab782d34984                            Important note \\n\\nDIE.   \n",
       "139458  ea714f84e062c442  The article did include his real name and it i...   \n",
       "69633   ba46606ee091e83b  \"\\nThx. Someone put it on Family of Barack Oba...   \n",
       "128559  af92f1e2f4c42372  \"\\nI apoligize for not being able to be \"\"full...   \n",
       "107815  4043aec662bda17c  listen peeps................this guy is one of...   \n",
       "71291   bed529ce6cb780a8  This addition is just as misleading as the sum...   \n",
       "156348  cccd45d35c7cd352  your tendancies to be editor of financial topi...   \n",
       "153271  9a204083856a48c1  fair enough. i'll go hide in a hole. good luck...   \n",
       "91919   f5bc78ba654ab185  Nagorno Karabakh \\n\\nThe following resource de...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "40740       1             0        1       0       1              0  \n",
       "103830      0             0        0       0       0              0  \n",
       "139458      0             0        0       0       0              0  \n",
       "69633       0             0        0       0       0              0  \n",
       "128559      0             0        0       0       0              0  \n",
       "107815      0             0        0       0       0              0  \n",
       "71291       0             0        0       0       0              0  \n",
       "156348      0             0        0       0       0              0  \n",
       "153271      0             0        0       0       0              0  \n",
       "91919       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values in numeric columns\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkout the negative labels percentage in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unlabelled comments is  89.83211235124176\n"
     ]
    }
   ],
   "source": [
    "unlabelled_in_all = train_df[(train_df['toxic']!=1) & (train_df['severe_toxic']!=1) & (train_df['obscene']!=1) & \n",
    "                            (train_df['threat']!=1) & (train_df['insult']!=1) & (train_df['identity_hate']!=1)]\n",
    "print('Percentage of unlabelled comments is ', len(unlabelled_in_all)/len(train_df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if and row doet not have comment_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_comment = train_df[train_df['comment_text'].isnull()]\n",
    "len(no_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, comment_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_comment = test_df[test_df['comment_text'].isnull()]\n",
    "no_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the labels and rows counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in test is 153164\n",
      "Total rows in train is 159571\n",
      "toxic            15294\n",
      "severe_toxic      1595\n",
      "obscene           8449\n",
      "threat             478\n",
      "insult            7877\n",
      "identity_hate     1405\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Total rows in test is {}'.format(len(test_df)))\n",
    "print('Total rows in train is {}'.format(len(train_df)))\n",
    "print(train_df[cols_target].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the labels distribution (i.e. the amount of 1s and 0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplt_df = train_df[cols_target].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "barplt_df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the average character length for the comments in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['char_length'] = train_df['comment_text'].apply(lambda x: len(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the histogram plot for text length\n",
    "sns.set()\n",
    "train_df['char_length'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg char length 394.0732213246768\n"
     ]
    }
   ],
   "source": [
    "print(\"avg char length {}\".format(sum(train_df['char_length'])/len(train_df['char_length'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Learning part (BiLSTM with Attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the structure of the model is shown below, a BiLSTM with Attention model was used to implement this task of multi-label classification\n",
    "\n",
    "\n",
    "\n",
    "#### Notes on the model\n",
    "\n",
    "* although the model achieves good accuracy in predicting each label in the dataset, other hpyerparameter and modifications might be required to make it better. (adding drop, decreasing the complexity etc..)\n",
    "* The attention layer performs poorly to detect the part the might trigger a label.\n",
    "* ⚠️ The task was to create a model that highlights each part of the comment that is responsible for each label predicted, however; i didn't understand this instead this model only focuses on the entire string and try to focus on all the parts that might be important to predict a label. This requested model might be created using a Multi-Headed Attention model (i'm still working on developing the model)\n",
    "* ⚠️ the dimenions in the figure doesn't match the trained model below, however the structure is the same, it was changed because the dimensions in the figure below caused the model to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./bilstm_attn.jpeg\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Preprocessing \n",
    " \n",
    "the tokenizer `PreTrainedTokenizerFast` was used to tokenize and transform the sequences into integer sequences which we will use later to index and Embedding Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(identifier):\n",
    "    \"\"\"this function splits CamelCased words and return them to the tokenizer\"\"\"\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_set(dataset, max_length=64):\n",
    "    \"\"\"returns input_ids, input_masks, labels for set of data ready in BERT format\"\"\"\n",
    "    global tokenizer\n",
    "    \n",
    "    input_ids = dataset\n",
    "#     for i in tqdm(dataset):\n",
    "#         input_ids.append(camel_case_split(i))\n",
    "    tokenized = tokenizer.batch_encode_plus(input_ids,return_token_type_ids=False, return_attention_mask=False, pad_to_max_length=True,truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "hidden_size=128\n",
    "tokenizer = None\n",
    "batch_size = 32\n",
    "n_epochs = 4\n",
    "embed_size = 100\n",
    "lr = 0.001\n",
    "model_path = \"BiLSTM.pt\"\n",
    "use_gpu = True\n",
    "dev_size = int(train_df.shape[0] * 0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking if there exists a GPU on the machine to use it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device config\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"a Single Attention Layer\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.feature_dim = feature_dim\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "    \n",
    "    def forward(self, x, step_dim, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1), torch.unsqueeze(a, -1)\n",
    "\n",
    "\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    \"\"\"the BiLSTM model refer to the image above to understand the structure of the model\"\"\"\n",
    "    def __init__(self,hidden_size,embed_size,max_features,num_classes,max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        \n",
    "#         self.embedding_dropout = dropout.SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size * 2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(768, 768)\n",
    "        self.linear2 = nn.Linear(768, 768)\n",
    "        \n",
    "        self.linear_out = nn.Linear(768, 1)\n",
    "        self.linear_aux_out = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, x, step_len):\n",
    "        h_embedding = self.embedding(x)\n",
    "#         print(f\"h_emdedding size : {h_embedding.shape}\")\n",
    "#         h_embedding = self.embedding_dropout(h_embedding)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "#         print(f\"lstm1 size : {h_lstm1.shape}\")\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "#         print(f\"lstm2 size : {h_lstm2.shape}\")\n",
    "        #Attention layer\n",
    "        h_lstm_atten, weights = self.lstm_attention(h_lstm2, max_length)\n",
    "#         print(f\"h_lstm , w sizes : {h_lstm_atten.shape}, {weights.shape}\")\n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "#         print(f\"avg pool : {avg_pool.shape}\")\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "#         print(f\"max pool : {max_pool.shape}\")\n",
    "        h_conc = torch.cat((h_lstm_atten, max_pool, avg_pool), 1)\n",
    "#         print(f\"h_conc : {h_conc.shape}\")\n",
    "        h_conc_linear1 = F.relu(self.linear1(h_conc))\n",
    "#         print(f\"h_conc_linear1 : {h_conc_linear1.shape}\")\n",
    "        h_conc_linear2 = F.relu(self.linear2(h_conc))\n",
    "#         print(f\"h_conc_linear2 : {h_conc_linear2.shape}\")\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        result = self.linear_out(hidden)\n",
    "#         print(f\"result : {result.shape}\")\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "#         print(f\"aux_result : {aux_result.shape}\")\n",
    "#         out = torch.cat([result, aux_result], 1)\n",
    "#         print(f\"out : {out.shape}\")\n",
    "#         return out, weights\n",
    "        return aux_result, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training Loop\n",
    "\n",
    "* mini batching was used to increase the performance\n",
    "* because this is a multi-label classification problem, a proper criterion needed to be chosen, in this case it was BCEWithLogitsLoss (which is the BCE with sigmoid implemented internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10,max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            \n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred,y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "            \n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "        \n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            \n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The evaulating/testing part\n",
    "\n",
    "* each time a mini batch is run through the model and the corresponding weigts are saved and column-summed to get the count of True Positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1,6))\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            \n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            \n",
    "            y_pred_labels = (torch.sigmoid(y_pred).detach().cpu().numpy() > 0.5)\n",
    "            \n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "            \n",
    "    return preds\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the datasets\n",
    "\n",
    "* a portion specified by dev_size is taken from the training dataframe to be used as a dev set, this would help us after each epoch to see how our model is doing (i.e. generalizing the model)\n",
    "* each data is tokenized and turned into sequence ids and converted into torch.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_cpy = train_df[dev_size:]\n",
    "dev_df_cpy = train_df[:dev_size]\n",
    "test_df_cpy = test_df\n",
    "y_true_cpy = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madbulattif18/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing training data...\n",
      "preprocessing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madbulattif18/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "print(\"preprocessing training data...\")\n",
    "X_train = prepare_set(train_df_cpy['comment_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_dev = prepare_set(dev_df_cpy['comment_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing test data...\")\n",
    "test_df = pd.merge(test_df_cpy,y_true_cpy,on='id')\n",
    "# -1 labels mean that those lines were not used for the scoring \n",
    "test_df = test_df[test_df[\"toxic\"] >= 0]\n",
    "\n",
    "X_test = prepare_set(test_df['comment_text'].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df_cpy[cols_target].values\n",
    "y_dev = dev_df_cpy[cols_target].values\n",
    "y_test = test_df[cols_target].values\n",
    "\n",
    "\n",
    "x_train_torch = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "x_dev_torch = torch.tensor(X_dev, dtype=torch.long).to(device)\n",
    "x_test_torch = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train,dtype=torch.float).to(device)\n",
    "y_dev_torch = torch.tensor(y_dev, dtype=torch.float).to(device)\n",
    "# y_val_torch = torch.tensor(np.hstack([y_val, y_aux_val]), dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test,dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for training set\n",
    "train_data = TensorDataset(x_train_torch, y_train_torch)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for dev set\n",
    "dev_data = TensorDataset(x_dev_torch, y_dev_torch)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for dev set.\n",
    "test_data = TensorDataset(x_test_torch, y_test_torch)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMWithAttention(\n",
       "  (embedding): Embedding(30522, 100)\n",
       "  (lstm1): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (lstm2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "  (lstm_attention): Attention()\n",
       "  (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_out): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (linear_aux_out): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTMWithAttention(hidden_size=hidden_size,\n",
    "                            embed_size=embed_size,\n",
    "                            max_features=tokenizer.vocab_size,\n",
    "                            num_classes=6,\n",
    "                            max_length=max_length)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "if you want to train the model from scratch, umcomment the next cell, otherwise you might opt to load the pretrained model (link shown in the prerequsits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4488/4488 [01:03<00:00, 70.90it/s]\n",
      "  4%|▍         | 21/499 [00:00<00:02, 201.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:02<00:00, 204.12it/s]\n",
      "  0%|          | 5/4488 [00:00<01:33, 48.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the best model so far\n",
      "Epoch 1/4\t training_loss=0.0606 \t validation_loss= 0.053738 \t time=65.75s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4488/4488 [01:02<00:00, 71.42it/s]\n",
      "  4%|▍         | 21/499 [00:00<00:02, 200.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:02<00:00, 203.45it/s]\n",
      "  0%|          | 4/4488 [00:00<01:56, 38.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the best model so far\n",
      "Epoch 2/4\t training_loss=0.0421 \t validation_loss= 0.045444 \t time=65.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4488/4488 [01:10<00:00, 63.99it/s]\n",
      "  4%|▍         | 19/499 [00:00<00:02, 189.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:03<00:00, 164.04it/s]\n",
      "  0%|          | 5/4488 [00:00<01:40, 44.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\t training_loss=0.0341 \t validation_loss= 0.048474 \t time=73.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4488/4488 [01:03<00:00, 71.08it/s]\n",
      "  4%|▍         | 21/499 [00:00<00:02, 200.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:02<00:00, 203.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4\t training_loss=0.0267 \t validation_loss= 0.051875 \t time=65.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.06061140655270359,\n",
       "  0.042105396785314705,\n",
       "  0.034075382167296754,\n",
       "  0.026715138823414936],\n",
       " [0.0537381234853146,\n",
       "  0.045444340547285425,\n",
       "  0.04847354499161486,\n",
       "  0.051875007179596554])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_model(model=model,loss_fn=None,lr=lr,batch_size=batch_size,n_epochs=n_epochs,max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMWithAttention(\n",
       "  (embedding): Embedding(30522, 100)\n",
       "  (lstm1): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (lstm2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "  (lstm_attention): Attention()\n",
       "  (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear_out): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (linear_aux_out): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 217.86it/s]\n"
     ]
    }
   ],
   "source": [
    "true_positives = evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic accuracy is 91.68%\n",
      "severe_toxic accuracy is 99.40%\n",
      "obscene accuracy is 96.02%\n",
      "threat accuracy is 99.71%\n",
      "insult accuracy is 96.41%\n",
      "identity_hate accuracy is 99.09%\n"
     ]
    }
   ],
   "source": [
    "for i,acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_color_arr = [(162, 155, 254),(223, 230, 233),\n",
    "                 (129, 236, 236),(0, 206, 201),\n",
    "                 (250, 177, 160),(255, 234, 167),\n",
    "                 (250, 177, 160),(253, 203, 110),\n",
    "                 (225, 112, 85), (99, 110, 114)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_escape(r, g, b, background=False):\n",
    "    return '\\033[{};2;{};{};{}m'.format(48 if background else 38, r, g, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorText(token,red,green,blue):\n",
    "    return get_color_escape(0, 0, 0) + get_color_escape(red, green, blue, True)+ token + '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_input():\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            str = input(\"Enter a String:\")\n",
    "            if str == 'quit':\n",
    "                break\n",
    "            input_ids = prepare_set([str])\n",
    "            single_x = torch.tensor(input_ids,dtype=torch.long).to(device)\n",
    "            ys, attention_w = model(single_x,max_length)\n",
    "            \n",
    "            ys = torch.sigmoid(ys).detach().cpu().numpy().flatten()\n",
    "            attention_w = attention_w.detach().cpu().numpy().flatten()\n",
    "            \n",
    "            decoded_str = tokenizer.decode(*input_ids).split()\n",
    "\n",
    "            attention_ids = [decoded_str.index(w) for w in decoded_str if (w != \"[CLS]\" and w != \"[SEP]\" and w != \"[PAD]\")]\n",
    "            \n",
    "            print(f\"original text: {str}\")\n",
    "            \n",
    "            highlighted_str = []\n",
    "            color_index = 0\n",
    "\n",
    "            for _id in attention_ids:\n",
    "                if attention_w[_id] > 0.035:\n",
    "                    highlighted_str.append(colorText(decoded_str[_id]\n",
    "                                                   ,rgb_color_arr[color_index][0]\n",
    "                                                   ,rgb_color_arr[color_index][1]\n",
    "                                                   ,rgb_color_arr[color_index][2]))\n",
    "        \n",
    "                else:\n",
    "                    highlighted_str.append(decoded_str[_id])\n",
    "                color_index+=1\n",
    "                if(color_index>=10):\n",
    "                    color_index=0\n",
    "            \n",
    "            print(f\"String after attention and multi-label classification: { ' '.join(highlighted_str)}\")\n",
    "            print(\"labels: =======\")\n",
    "            print(', '.join('\"{0}:{1:.2f}%\"'.format(cols_target[i],(w*100)) for i,w in enumerate(ys)))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a String:the writer should quit his job and kill himself\n",
      "original text: the writer should quit his job and kill himself\n",
      "[0.01462905 0.02015642 0.01929706 0.04071356 0.04440593 0.0275556\n",
      " 0.02992115 0.064064   0.03321201]\n",
      "String after attention and multi-label classification: the writer should \u001b[38;2;0;0;0m\u001b[48;2;0;206;201mquit\u001b[0m \u001b[38;2;0;0;0m\u001b[48;2;250;177;160mhis\u001b[0m job and \u001b[38;2;0;0;0m\u001b[48;2;253;203;110mkill\u001b[0m himself\n",
      "labels: =======\n",
      "\"toxic:41.58%\", \"severe_toxic:0.40%\", \"obscene:1.31%\", \"threat:6.84%\", \"insult:3.13%\", \"identity_hate:1.59%\"\n",
      "\n",
      "\n",
      "Enter a String:quit\n"
     ]
    }
   ],
   "source": [
    "predict_user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
